{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7cc78e",
   "metadata": {},
   "source": [
    "\n",
    "# WIRE Wallet — Scaling Pipelines (MinMax vs Standard vs Hybrid)\n",
    "\n",
    "This notebook loads your engineered dataset and produces three scaled versions:\n",
    "\n",
    "1. **All-MinMax** → `_minmax.csv` (baseline / current)\n",
    "2. **All-Standard** → `_standard.csv`\n",
    "3. **Hybrid (log+Robust + Standard + Passthrough + cyclical)** → `_hybrid.csv` (**recommended**)\n",
    "\n",
    "It also persists the fitted preprocessors with joblib so you can reuse them in your training and serving pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb12f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Configuration ====\n",
    "INPUT_CSV = \"../../data/combined_wallets_with_transactions_and_balances_2.csv\"  # change if needed\n",
    "OUTPUT_MINMAX = \"../../data/combined_wallets_with_transactions_and_balances_minmax.csv\"\n",
    "OUTPUT_STANDARD = \"../../data/combined_wallets_with_transactions_and_balances_standard.csv\"\n",
    "OUTPUT_HYBRID = \"../../data/combined_wallets_with_transactions_and_balances_hybrid.csv\"\n",
    "\n",
    "SCALER_MINMAX_PATH = \"preprocessor_minmax.pkl\"\n",
    "SCALER_STANDARD_PATH = \"preprocessor_standard.pkl\"\n",
    "SCALER_HYBRID_PATH = \"preprocessor_hybrid.pkl\"\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edcc0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# For optional quick plots (Fig-7-like feature sanity checks)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf49c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../../data/combined_wallets_with_transactions_and_balances_2.csv\n",
      "Rows, Cols: (33749, 34)\n",
      "\n",
      "Columns: ['Address', 'Name', 'isSafe', 'Tags', 'Transactions', 'noOfTrx.1', 'Balance', 'total_transactions', 'self_transfer_ratio', 'circular_txn_count', 'circular_txn_ratio', 'avg_txn_value_eth', 'txn_spike_score', 'value_std_dev', 'avg_gas_used', 'avg_gas_price', 'active_days', 'wallet_age_days', 'unique_counterparties', 'failed_txn_ratio', 'eth_inflow_outflow_ratio', 'erc20_txn_count', 'nft_txn_count', 'first_txn_time_of_day', 'erc20_token_diversity', 'tx_direction_ratio', 'contract_interaction_ratio', 'value_entropy', 'tx_burst_count', 'average_txn_interval', 'new_token_interaction_count', 'token_approval_count', 'sbt_poap_event_count', 'approved_token_list']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Load ====\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "print(\"Loaded:\", INPUT_CSV)\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "\n",
    "# Ensure isSafe exists if present, but it's not a feature to scale\n",
    "label_col = \"isSafe\" if \"isSafe\" in df.columns else None\n",
    "\n",
    "# Keep a copy of raw for passthrough of non-feature columns at the end\n",
    "raw_df = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9321db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label column detected: isSafe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Feature Columns (from your message) ====\n",
    "all_features = [\n",
    "    'noOfTrx.1', 'Balance', 'self_transfer_ratio', 'circular_txn_count',\n",
    "    'circular_txn_ratio', 'avg_txn_value_eth', 'txn_spike_score',\n",
    "    'value_std_dev', 'avg_gas_used', 'avg_gas_price', 'active_days',\n",
    "    'wallet_age_days', 'unique_counterparties', 'failed_txn_ratio',\n",
    "    'eth_inflow_outflow_ratio', 'erc20_txn_count', 'nft_txn_count',\n",
    "    'first_txn_time_of_day', 'erc20_token_diversity', 'tx_direction_ratio',\n",
    "    'contract_interaction_ratio', 'value_entropy', 'average_txn_interval',\n",
    "    'new_token_interaction_count', 'token_approval_count',\n",
    "    'sbt_poap_event_count'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the file (avoid KeyErrors if some aren't present)\n",
    "features = [c for c in all_features if c in df.columns]\n",
    "\n",
    "missing = [c for c in all_features if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"Warning: missing columns (will be skipped):\", missing)\n",
    "\n",
    "# Separate features from non-features\n",
    "non_feature_cols = [c for c in df.columns if c not in features]\n",
    "if label_col and label_col in non_feature_cols:\n",
    "    print(\"Label column detected:\", label_col)\n",
    "\n",
    "X = df[features].copy()\n",
    "\n",
    "# Fill NA robustly so scalers don't crash\n",
    "X = X.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939f4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid groups:\n",
      "  log+robust: ['Balance', 'noOfTrx.1', 'avg_txn_value_eth', 'value_std_dev', 'average_txn_interval']\n",
      "  standard  : ['active_days', 'wallet_age_days', 'unique_counterparties', 'avg_gas_used', 'avg_gas_price']\n",
      "  cyclical  : ['first_txn_time_of_day'] -> will expand to 2 cols (sin, cos) if present\n",
      "  passthrough: ['self_transfer_ratio', 'circular_txn_count', 'circular_txn_ratio', 'txn_spike_score', 'failed_txn_ratio', 'eth_inflow_outflow_ratio', 'erc20_txn_count', 'nft_txn_count', 'erc20_token_diversity', 'tx_direction_ratio', 'contract_interaction_ratio', 'value_entropy', 'new_token_interaction_count', 'token_approval_count', 'sbt_poap_event_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Preprocessors ====\n",
    "\n",
    "# A) All-MinMax on features\n",
    "pre_minmax = ColumnTransformer([\n",
    "    (\"mm\", MinMaxScaler(), features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# B) All-Standard on features\n",
    "pre_standard = ColumnTransformer([\n",
    "    (\"std\", StandardScaler(), features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# C) Hybrid pipeline (recommended)\n",
    "# - Log + RobustScaler: heavy-tailed numerics\n",
    "log_robust_cols = [c for c in [\"Balance\",\"noOfTrx.1\",\"avg_txn_value_eth\",\"value_std_dev\",\"average_txn_interval\"] if c in features]\n",
    "# - StandardScaler: mid-skew continuous\n",
    "std_cols = [c for c in [\"active_days\",\"wallet_age_days\",\"unique_counterparties\",\"avg_gas_used\",\"avg_gas_price\"] if c in features]\n",
    "# - Cyclical: first_txn_time_of_day (if present)\n",
    "cyc_cols = [c for c in [\"first_txn_time_of_day\"] if c in features]\n",
    "# - Passthrough (ratios & sparse counts)\n",
    "cyc_and_scaled = set(log_robust_cols + std_cols + cyc_cols)\n",
    "passthrough_cols = [c for c in features if c not in cyc_and_scaled]\n",
    "\n",
    "log_then_robust = Pipeline([\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"robust\", RobustScaler())\n",
    "])\n",
    "\n",
    "# Encode hour-of-day as sin/cos pair\n",
    "def hour_to_sin_cos(x):\n",
    "    arr = np.asarray(x).astype(float)\n",
    "    # Protect against out-of-range or negative hours\n",
    "    arr = np.clip(arr, 0, 23)\n",
    "    sin = np.sin(2*np.pi*arr/24)\n",
    "    cos = np.cos(2*np.pi*arr/24)\n",
    "    return np.c_[sin, cos]\n",
    "\n",
    "cyclical = FunctionTransformer(hour_to_sin_cos, validate=False)\n",
    "\n",
    "pre_hybrid = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"log_robust\", log_then_robust, log_robust_cols),\n",
    "        (\"standard\", StandardScaler(), std_cols),\n",
    "        (\"cyclical\", cyclical, cyc_cols),\n",
    "        (\"passthrough\", \"passthrough\", passthrough_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "print(\"Hybrid groups:\")\n",
    "print(\"  log+robust:\", log_robust_cols)\n",
    "print(\"  standard  :\", std_cols)\n",
    "print(\"  cyclical  :\", cyc_cols, \"-> will expand to 2 cols (sin, cos) if present\")\n",
    "print(\"  passthrough:\", passthrough_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76621eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_transform_and_export(preprocessor, X, base_df, out_csv, scaler_path):\n",
    "    # Fit and transform features\n",
    "    X_t = preprocessor.fit_transform(X)\n",
    "    # Build column names for the transformed matrix\n",
    "    out_cols = []\n",
    "    # For the hybrid, cyclical expands to two columns; we will name them explicitly\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name == \"cyclical\" and len(cols) > 0:\n",
    "            for c in cols:\n",
    "                out_cols += [f\"{c}_sin\", f\"{c}_cos\"]\n",
    "        elif name in (\"log_robust\",\"standard\",\"mm\",\"std\",\"passthrough\"):\n",
    "            # keep original names for these\n",
    "            if isinstance(cols, list):\n",
    "                out_cols += cols\n",
    "            else:\n",
    "                # Column indices or slice - fallback generic names\n",
    "                out_cols += [f\"{name}_{i}\" for i in range(np.array(X_t).shape[1] - len(out_cols))]\n",
    "    X_t = pd.DataFrame(X_t, columns=out_cols, index=base_df.index)\n",
    "\n",
    "    # Re-attach non-feature columns (Address, tags, labels etc.)\n",
    "    out = pd.concat([base_df.drop(columns=[c for c in features if c in base_df.columns], errors=\"ignore\"), X_t], axis=1)\n",
    "\n",
    "    # Save artifacts\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    joblib.dump(preprocessor, scaler_path)\n",
    "\n",
    "    print(f\"Saved scaled CSV -> {out_csv}\")\n",
    "    print(f\"Saved fitted preprocessor -> {scaler_path}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting All-MinMax ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Fitting All-MinMax ===\")\n",
    "out_minmax = fit_transform_and_export(pre_minmax, X, raw_df, OUTPUT_MINMAX, SCALER_MINMAX_PATH)\n",
    "\n",
    "print(\"\\n=== Fitting All-Standard ===\")\n",
    "out_standard = fit_transform_and_export(pre_standard, X, raw_df, OUTPUT_STANDARD, SCALER_STANDARD_PATH)\n",
    "\n",
    "print(\"\\n=== Fitting HYBRID (recommended) ===\")\n",
    "out_hybrid = fit_transform_and_export(pre_hybrid, X, raw_df, OUTPUT_HYBRID, SCALER_HYBRID_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f133768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: quick Fig-7-like checks on one or two heavy-tailed features\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(out_minmax.get('Balance', pd.Series(dtype=float)), bins=30)\n",
    "plt.title('Balance (MinMax)')\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(out_standard.get('Balance', pd.Series(dtype=float)), bins=30)\n",
    "plt.title('Balance (Standard)')\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(out_hybrid.get('Balance', pd.Series(dtype=float)), bins=30)\n",
    "plt.title('Balance (Hybrid)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
