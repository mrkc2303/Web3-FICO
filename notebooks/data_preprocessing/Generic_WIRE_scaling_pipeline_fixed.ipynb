{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904c55e9",
   "metadata": {},
   "source": [
    "\n",
    "# WIRE Wallet — Scaling Pipelines (MinMax vs Standard vs Hybrid)\n",
    "\n",
    "This notebook loads your engineered dataset and produces three scaled versions:\n",
    "\n",
    "1. **All-MinMax** → `_minmax.csv` (baseline / current)  \n",
    "2. **All-Standard** → `_standard.csv`  \n",
    "3. **Hybrid (log+Robust + Standard + Passthrough + cyclical)** → `_hybrid.csv` (**recommended**)  \n",
    "\n",
    "It also persists the fitted preprocessors with joblib so you can reuse them in your training and serving pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45f2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Configuration ====\n",
    "INPUT_CSV = \"../../data/combined_wallets_with_transactions_and_balances_2.csv\"  # change if needed\n",
    "OUTPUT_MINMAX = \"../../data/combined_wallets_with_transactions_and_balances_minmax.csv\"\n",
    "OUTPUT_STANDARD = \"../../data/combined_wallets_with_transactions_and_balances_standard.csv\"\n",
    "OUTPUT_HYBRID = \"../../data/combined_wallets_with_transactions_and_balances_hybrid.csv\"\n",
    "\n",
    "SCALER_MINMAX_PATH = \"../../models/preprocessor_minmax.pkl\"\n",
    "SCALER_STANDARD_PATH = \"../../models/preprocessor_standard.pkl\"\n",
    "SCALER_HYBRID_PATH = \"../../models/preprocessor_hybrid.pkl\"\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8892d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95ae71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../../data/combined_wallets_with_transactions_and_balances_2.csv\n",
      "Rows, Cols: (33749, 34)\n",
      "\n",
      "Columns: ['Address', 'Name', 'isSafe', 'Tags', 'Transactions', 'noOfTrx.1', 'Balance', 'total_transactions', 'self_transfer_ratio', 'circular_txn_count', 'circular_txn_ratio', 'avg_txn_value_eth', 'txn_spike_score', 'value_std_dev', 'avg_gas_used', 'avg_gas_price', 'active_days', 'wallet_age_days', 'unique_counterparties', 'failed_txn_ratio', 'eth_inflow_outflow_ratio', 'erc20_txn_count', 'nft_txn_count', 'first_txn_time_of_day', 'erc20_token_diversity', 'tx_direction_ratio', 'contract_interaction_ratio', 'value_entropy', 'tx_burst_count', 'average_txn_interval', 'new_token_interaction_count', 'token_approval_count', 'sbt_poap_event_count', 'approved_token_list']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Load ====\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(\"Loaded:\", INPUT_CSV)\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "\n",
    "label_col = \"isSafe\" if \"isSafe\" in df.columns else None\n",
    "raw_df = df.copy()\n",
    "\n",
    "# ==== Feature Columns ====\n",
    "all_features = [\n",
    "    'noOfTrx.1', 'Balance', 'self_transfer_ratio', 'circular_txn_count',\n",
    "    'circular_txn_ratio', 'avg_txn_value_eth', 'txn_spike_score',\n",
    "    'value_std_dev', 'avg_gas_used', 'avg_gas_price', 'active_days',\n",
    "    'wallet_age_days', 'unique_counterparties', 'failed_txn_ratio',\n",
    "    'eth_inflow_outflow_ratio', 'erc20_txn_count', 'nft_txn_count',\n",
    "    'first_txn_time_of_day', 'erc20_token_diversity', 'tx_direction_ratio',\n",
    "    'contract_interaction_ratio', 'value_entropy', 'average_txn_interval',\n",
    "    'new_token_interaction_count', 'token_approval_count',\n",
    "    'sbt_poap_event_count'\n",
    "]\n",
    "features = [c for c in all_features if c in df.columns]\n",
    "missing = [c for c in all_features if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"Warning: missing columns (will be skipped):\", missing)\n",
    "\n",
    "# Ensure numeric features before any scaling\n",
    "X = df[features].apply(pd.to_numeric, errors='coerce').fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bd8573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid groups:\n",
      "  log+robust: ['Balance', 'noOfTrx.1', 'avg_txn_value_eth', 'value_std_dev', 'average_txn_interval']\n",
      "  standard  : ['active_days', 'wallet_age_days', 'unique_counterparties', 'avg_gas_used', 'avg_gas_price']\n",
      "  cyclical  : ['first_txn_time_of_day'] -> (sin, cos)\n",
      "  passthrough: ['self_transfer_ratio', 'circular_txn_count', 'circular_txn_ratio', 'txn_spike_score', 'failed_txn_ratio', 'eth_inflow_outflow_ratio', 'erc20_txn_count', 'nft_txn_count', 'erc20_token_diversity', 'tx_direction_ratio', 'contract_interaction_ratio', 'value_entropy', 'new_token_interaction_count', 'token_approval_count', 'sbt_poap_event_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Safe numeric functions for FunctionTransformer ---\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def safe_log1p(X):\n",
    "    X = pd.DataFrame(X)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "    X = np.nan_to_num(X.to_numpy(dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X = np.clip(X, 0.0, None)\n",
    "    return np.log1p(X)\n",
    "\n",
    "def hour_to_sin_cos(X):\n",
    "    X = pd.DataFrame(X).apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "    arr = np.nan_to_num(X.to_numpy(dtype=float), nan=0.0)\n",
    "    arr = np.clip(arr, 0, 23)\n",
    "    sin = np.sin(2*np.pi*arr/24.0)\n",
    "    cos = np.cos(2*np.pi*arr/24.0)\n",
    "    return np.concatenate([sin, cos], axis=1)\n",
    "\n",
    "# Column groups\n",
    "log_robust_cols = [c for c in [\"Balance\",\"noOfTrx.1\",\"avg_txn_value_eth\",\"value_std_dev\",\"average_txn_interval\"] if c in features]\n",
    "std_cols        = [c for c in [\"active_days\",\"wallet_age_days\",\"unique_counterparties\",\"avg_gas_used\",\"avg_gas_price\"] if c in features]\n",
    "cyc_cols        = [c for c in [\"first_txn_time_of_day\"] if c in features]\n",
    "cyc_and_scaled  = set(log_robust_cols + std_cols + cyc_cols)\n",
    "passthrough_cols = [c for c in features if c not in cyc_and_scaled]\n",
    "\n",
    "log_then_robust = Pipeline([\n",
    "    (\"log\", FunctionTransformer(safe_log1p, validate=False)),\n",
    "    (\"robust\", RobustScaler())\n",
    "])\n",
    "cyclical = FunctionTransformer(hour_to_sin_cos, validate=False)\n",
    "\n",
    "# Three preprocessors\n",
    "pre_minmax = ColumnTransformer([(\"mm\", MinMaxScaler(), features)], remainder=\"drop\")\n",
    "pre_standard = ColumnTransformer([(\"std\", StandardScaler(), features)], remainder=\"drop\")\n",
    "pre_hybrid = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"log_robust\", log_then_robust, log_robust_cols),\n",
    "        (\"standard\", StandardScaler(), std_cols),\n",
    "        (\"cyclical\", cyclical, cyc_cols),\n",
    "        (\"passthrough\", \"passthrough\", passthrough_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "print(\"Hybrid groups:\")\n",
    "print(\"  log+robust:\", log_robust_cols)\n",
    "print(\"  standard  :\", std_cols)\n",
    "print(\"  cyclical  :\", cyc_cols, \"-> (sin, cos)\")\n",
    "print(\"  passthrough:\", passthrough_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a07f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def fit_transform_and_export(preprocessor, X, base_df, out_csv, scaler_path):\n",
    "    Xt = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Clean feature names: strip transformer prefixes like \"mm__\", \"std__\", \"log_robust__\"\n",
    "    try:\n",
    "        raw_names = list(preprocessor.get_feature_names_out())\n",
    "        out_cols = []\n",
    "        for n in raw_names:\n",
    "            if n.startswith(\"cyclical__first_txn_time_of_day\"):\n",
    "                # last char is 0 or 1\n",
    "                out_cols.append(\"first_txn_time_of_day_sin\" if n.endswith(\"0\") else \"first_txn_time_of_day_cos\")\n",
    "            else:\n",
    "                out_cols.append(n.split(\"__\")[-1])  # keep only the base column name\n",
    "    except Exception:\n",
    "        out_cols = [f\"f_{i}\" for i in range(np.asarray(Xt).shape[1])]\n",
    "\n",
    "    Xt_df = pd.DataFrame(Xt, index=base_df.index, columns=out_cols)\n",
    "\n",
    "    out = pd.concat(\n",
    "        [base_df.drop(columns=[c for c in features if c in base_df.columns], errors=\"ignore\"), Xt_df],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    from pathlib import Path\n",
    "    Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(scaler_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    import joblib\n",
    "    joblib.dump(preprocessor, scaler_path)\n",
    "\n",
    "    print(f\"Saved scaled CSV -> {out_csv}\")\n",
    "    print(f\"Saved fitted preprocessor -> {scaler_path}\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8a63d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting All-MinMax ===\n",
      "Saved scaled CSV -> ../../data/combined_wallets_with_transactions_and_balances_minmax.csv\n",
      "Saved fitted preprocessor -> ../../models/preprocessor_minmax.pkl\n",
      "\n",
      "=== Fitting All-Standard ===\n",
      "Saved scaled CSV -> ../../data/combined_wallets_with_transactions_and_balances_standard.csv\n",
      "Saved fitted preprocessor -> ../../models/preprocessor_standard.pkl\n",
      "\n",
      "=== Fitting HYBRID (recommended) ===\n",
      "Saved scaled CSV -> ../../data/combined_wallets_with_transactions_and_balances_hybrid.csv\n",
      "Saved fitted preprocessor -> ../../models/preprocessor_hybrid.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Fitting All-MinMax ===\")\n",
    "out_minmax = fit_transform_and_export(pre_minmax, X, raw_df, OUTPUT_MINMAX, SCALER_MINMAX_PATH)\n",
    "\n",
    "print(\"\\n=== Fitting All-Standard ===\")\n",
    "out_standard = fit_transform_and_export(pre_standard, X, raw_df, OUTPUT_STANDARD, SCALER_STANDARD_PATH)\n",
    "\n",
    "print(\"\\n=== Fitting HYBRID (recommended) ===\")\n",
    "out_hybrid = fit_transform_and_export(pre_hybrid, X, raw_df, OUTPUT_HYBRID, SCALER_HYBRID_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b474eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba906d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
